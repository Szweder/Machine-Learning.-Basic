{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Линейная алгебра — Задание 1\n","\n","Даны матрицы:\n","\n","A =\n","\\[\n","\\begin{pmatrix}\n","2 & -4 \\\\\n","3 & 5 \\\\\n","-1 & 0 \\\\\n","\\end{pmatrix}\n","\\],  \n","B =\n","\\[\n","\\begin{pmatrix}\n","1 & 2 & 7 \\\\\n","-3 & -4 & 0 \\\\\n","5 & 2 & 1 \\\\\n","\\end{pmatrix}\n","\\],  \n","C =\n","\\[\n","\\begin{pmatrix}\n","6 & -3 & 9 \\\\\n","4 & -5 & 2 \\\\\n","8 & 1 & 5 \\\\\n","\\end{pmatrix}\n","\\]\n","\n","Найти:\n","\n","\\[\n","D = A^\\top C - 2A^\\top B^\\top\n","\\]\n","\n","Привести полную последовательность вычислений.\n"],"metadata":{"id":"369EiyN1qFXg"}},{"cell_type":"code","source":["import numpy as np\n","\n","A = np.array([[2, -4],\n","              [3, 5],\n","              [-1, 0]])\n","\n","B = np.array([[1, 2, 7],\n","              [-3, -4, 0],\n","              [5, 2, 1]])\n","\n","C = np.array([[6, -3, 9],\n","              [4, -5, 2],\n","              [8, 1, 5]])\n","\n","A_T = A.T\n","B_T = B.T\n","D = A_T @ C - 2 * (A_T @ B_T)\n","\n","print(\"A^T:\")\n","print(A_T)\n","print(\"\\nB^T:\")\n","print(B_T)\n","print(\"\\nA^T * C:\")\n","print(A_T @ C)\n","print(\"\\n2 * A^T * B^T:\")\n","print(2 * (A_T @ B_T))\n","print(\"\\nD = A^T * C - 2 * A^T * B^T:\")\n","print(D)\n"],"metadata":{"id":"1btUZIAeyB1o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Линейная алгебра — Задание 2\n","\n","Найти определитель матрицы:\n","\n","\\[\n","M =\n","\\begin{pmatrix}\n","1 & 2 & 3 \\\\\n","4 & 5 & 6 \\\\\n","7 & 8 & 9 \\\\\n","\\end{pmatrix}\n","\\]\n","\n","Сделать вывод.\n"],"metadata":{"id":"-IRuENLmzCHg"}},{"cell_type":"code","source":["M = np.array([[1, 2, 3],\n","              [4, 5, 6],\n","              [7, 8, 9]])\n","\n","det_M = np.linalg.det(M)\n","print(\"Определитель матрицы M:\", det_M)\n","\n","# Вывод\n","if np.isclose(det_M, 0):\n","    print(\"Вывод: матрица вырождена (сингулярна), определитель равен 0.\")\n","else:\n","    print(\"Матрица невырождена.\")\n"],"metadata":{"id":"MeIpsU45zDYY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Линейная алгебра — Задание 3\n","\n","Найти собственные значения и собственные векторы матрицы:\n","\n","\\[\n","A =\n","\\begin{pmatrix}\n","2 & 0 \\\\\n","0 & 3 \\\\\n","\\end{pmatrix}\n","\\]\n","\n","Сделать вывод.\n"],"metadata":{"id":"xwoNUbTezKlA"}},{"cell_type":"code","source":["A = np.array([[2, 0],\n","              [0, 3]])\n","\n","eig_vals, eig_vecs = np.linalg.eig(A)\n","\n","print(\"Собственные значения:\", eig_vals)\n","print(\"Собственные векторы:\\n\", eig_vecs)\n","\n","# Вывод\n","print(\"\\nВывод:\")\n","print(\"Матрица A — диагональная, её собственные значения — это элементы на диагонали.\")\n","print(\"Собственные векторы — ортогональные единичные векторы по осям координат.\")\n"],"metadata":{"id":"tzU4DycQzLbY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Производная и градиент — Задание 4\n","\n","Найти производную функции:\n","\n","\\[\n","f(x) = x^2 + 3x + 5\n","\\]\n","\n","Найти градиент функции:\n","\n","\\[\n","f(x, y) = x^2 + y^2 + xy\n","\\]\n","\n","Сделать вывод.\n"],"metadata":{"id":"4XdbWIgyzNnY"}},{"cell_type":"code","source":["import sympy as sp\n","\n","# Производная\n","x = sp.symbols('x')\n","f = x**2 + 3*x + 5\n","f_prime = sp.diff(f, x)\n","\n","print(\"Производная функции f(x) = x^2 + 3x + 5:\", f_prime)\n","\n","# Градиент\n","x, y = sp.symbols('x y')\n","f_xy = x**2 + y**2 + x*y\n","grad_f = [sp.diff(f_xy, var) for var in (x, y)]\n","\n","print(\"Градиент функции f(x, y) = x^2 + y^2 + xy:\", grad_f)\n","\n","# Вывод\n","print(\"\\nВывод:\")\n","print(\"Производная функции по x показывает скорость изменения вдоль оси x.\")\n","print(\"Градиент — вектор из частных производных, указывающий направление наибольшего роста функции.\")\n"],"metadata":{"id":"YrZ0BH6DzdiY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Метод Наименьших Квадратов — Задание 5\n","\n","Используя линейную модель вида:\n","\n","\\[\n","y = \\theta_0 + \\theta_1 x\n","\\]\n","\n","и заданные значения:\n","\n","\\[\n","x = [1, 2, 3, 4, 5] \\quad y = [1.2, 1.9, 3.0, 4.1, 5.1]\n","\\]\n","\n","Найти параметры \\(\\theta_0\\) и \\(\\theta_1\\) с помощью формулы:\n","\n","\\[\n","\\theta = (X^T X)^{-1} X^T y\n","\\]\n","\n","И построить график линии регрессии.\n"],"metadata":{"id":"NJNS5WMQzgCg"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Данные\n","x = np.array([1, 2, 3, 4, 5])\n","y = np.array([1.2, 1.9, 3.0, 4.1, 5.1])\n","\n","# Формируем матрицу признаков X с единицами для theta_0\n","X = np.vstack([np.ones(len(x)), x]).T\n","\n","# Метод наименьших квадратов\n","theta = np.linalg.inv(X.T @ X) @ X.T @ y\n","theta_0, theta_1 = theta\n","\n","print(f\"Найденные параметры: θ₀ = {theta_0:.4f}, θ₁ = {theta_1:.4f}\")\n","\n","# Построение графика\n","plt.scatter(x, y, label=\"Исходные данные\", color=\"blue\")\n","plt.plot(x, theta_0 + theta_1 * x, label=\"Линия регрессии\", color=\"red\")\n","plt.xlabel(\"x\")\n","plt.ylabel(\"y\")\n","plt.title(\"Метод Наименьших Квадратов\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"84YjNQ6SzlWA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Задание 6: Сравнение с LinearRegression\n","\n","Проверим, совпадают ли параметры, найденные вручную методом наименьших квадратов, с результатами библиотеки `sklearn.linear_model.LinearRegression`.\n"],"metadata":{"id":"dADBD-yJzsZY"}},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\n","\n","# Модель и обучение\n","model = LinearRegression()\n","model.fit(x.reshape(-1, 1), y)\n","\n","# Полученные коэффициенты\n","print(f\"θ₀ (intercept_): {model.intercept_:.4f}\")\n","print(f\"θ₁ (coef_): {model.coef_[0]:.4f}\")\n","\n","# Сравнение графиков\n","plt.scatter(x, y, label=\"Исходные данные\", color=\"blue\")\n","plt.plot(x, model.predict(x.reshape(-1, 1)), label=\"sklearn LinearRegression\", color=\"green\", linestyle=\"--\")\n","plt.plot(x, theta_0 + theta_1 * x, label=\"МНК вручную\", color=\"red\")\n","plt.xlabel(\"x\")\n","plt.ylabel(\"y\")\n","plt.title(\"Сравнение: МНК вручную и LinearRegression\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"vQ_aRvVwzu4I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Задание 7: Полиномиальная регрессия и МНК\n","\n","Рассмотрим случай, когда зависимость между признаками и целевой переменной нелинейная.  \n","Для этого сгенерируем данные, соответствующие квадратичной зависимости, и применим полиномиальную регрессию.  \n","Мы будем использовать метод наименьших квадратов, как и раньше, но с расширенной матрицей признаков (добавим столбец $x^2$).\n"],"metadata":{"id":"b_myFhBizyZw"}},{"cell_type":"code","source":["# Генерация данных с квадратичной зависимостью\n","np.random.seed(42)\n","x_poly = np.linspace(-3, 3, 100)\n","y_poly = 4 + 2 * x_poly + 3 * x_poly**2 + np.random.randn(100) * 3\n","\n","# Формируем матрицу признаков: [1, x, x^2]\n","X_design = np.vstack((np.ones_like(x_poly), x_poly, x_poly**2)).T\n","\n","# Вычисление коэффициентов МНК для полиномиальной регрессии\n","theta_poly = np.linalg.inv(X_design.T @ X_design) @ X_design.T @ y_poly\n","\n","# Построение предсказаний\n","y_hat_poly = X_design @ theta_poly\n","\n","# Вывод коэффициентов\n","print(\"Найденные параметры θ:\")\n","print(f\"θ₀ = {theta_poly[0]:.4f}, θ₁ = {theta_poly[1]:.4f}, θ₂ = {theta_poly[2]:.4f}\")\n","\n","# Визуализация\n","plt.scatter(x_poly, y_poly, label=\"Данные\", alpha=0.7)\n","plt.plot(x_poly, y_hat_poly, color=\"red\", label=\"Полиномиальная регрессия (МНК)\")\n","plt.title(\"Полиномиальная регрессия (степень 2)\")\n","plt.xlabel(\"x\")\n","plt.ylabel(\"y\")\n","plt.grid(True)\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"Xxh5hoPO0BGh"},"execution_count":null,"outputs":[]}]}